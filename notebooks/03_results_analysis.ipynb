{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d712eff6",
   "metadata": {},
   "source": [
    "# Invoice Extraction: Results Analysis\n",
    "\n",
    "This notebook provides tools for analyzing and comparing extraction experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"results_analysis\")\n",
    "\n",
    "# Add the project root to the path if not already there\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import utility modules\n",
    "try:\n",
    "    from src.notebook.experiment_utils import load_experiment_results\n",
    "    from src.results.collector import ResultsCollector, ComparisonResult, ExperimentLoader, ExperimentComparator\n",
    "    from src.notebook.visualization_utils import NotebookVisualizationManager\n",
    "    utils_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Error importing utilities: {str(e)}\")\n",
    "    utils_available = False\n",
    "\n",
    "# Initialize visualization manager\n",
    "viz_manager = NotebookVisualizationManager() if utils_available else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88897e",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Results\n",
    "\n",
    "First, let's load the results from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for finding experiment results\n",
    "if utils_available:\n",
    "    # Create an experiment loader\n",
    "    experiment_loader = ExperimentLoader()\n",
    "    \n",
    "    # Discover available experiments\n",
    "    discovered_experiments = experiment_loader.discover_experiments(force_refresh=True)\n",
    "    \n",
    "    if discovered_experiments:\n",
    "        # Convert to DataFrame for easier viewing\n",
    "        exp_df = experiment_loader.get_experiment_dataframe()\n",
    "        \n",
    "        # Display experiment summary\n",
    "        print(f\"üìä Found {len(discovered_experiments)} experiments\")\n",
    "        print(\"\\nExperiment Summary:\")\n",
    "        display(exp_df[['experiment_id', 'model', 'date', 'fields', 'type']].head(10))\n",
    "        \n",
    "        if len(discovered_experiments) > 10:\n",
    "            print(f\"...and {len(discovered_experiments) - 10} more experiments\")\n",
    "    else:\n",
    "        print(\"No experiments found. Run an experiment first using the experiment configuration notebook.\")\n",
    "    \n",
    "    # Provide code for manually specifying experiment ID\n",
    "    print(\"\\nüí° To load a specific experiment by ID:\")\n",
    "    print(\"\"\"\n",
    "    # Example:\n",
    "    experiment_id = \"your_experiment_id_here\"\n",
    "    result, metadata = load_experiment_results(experiment_id)\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"Utilities not available, cannot load experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b261b",
   "metadata": {},
   "source": [
    "## 2. Select and Analyze an Experiment\n",
    "\n",
    "Choose an experiment to analyze from the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your experiment ID\n",
    "if utils_available:\n",
    "    # For demonstration - in a real notebook, users would select from the discovered experiments\n",
    "    # Either use the most recent experiment or allow the user to specify one\n",
    "    if discovered_experiments:\n",
    "        # Sort by date and take the most recent\n",
    "        most_recent_exp = sorted(discovered_experiments, key=lambda x: x.get('date', ''), reverse=True)[0]\n",
    "        experiment_id = most_recent_exp.get('experiment_id')\n",
    "        print(f\"Using most recent experiment: {experiment_id}\")\n",
    "    else:\n",
    "        # Example ID - users would replace this\n",
    "        experiment_id = \"example_experiment_id\"\n",
    "        print(f\"No experiments found. Using example ID: {experiment_id}\")\n",
    "        print(\"In a real notebook, replace this with your actual experiment ID\")\n",
    "    \n",
    "    try:\n",
    "        # Load the experiment results\n",
    "        result, metadata = load_experiment_results(experiment_id)\n",
    "        print(f\"‚úÖ Loaded experiment: {experiment_id}\")\n",
    "        \n",
    "        # Display experiment overview\n",
    "        print(\"\\nüìã Experiment Overview:\")\n",
    "        print(f\"‚Ä¢ Model: {metadata.get('model', 'N/A')}\")\n",
    "        print(f\"‚Ä¢ Date: {metadata.get('date', 'N/A')}\")\n",
    "        print(f\"‚Ä¢ Fields: {', '.join(metadata.get('fields', []))}\")\n",
    "        print(f\"‚Ä¢ Type: {metadata.get('type', 'N/A')}\")\n",
    "        \n",
    "        # You'd use interactive widgets in a real notebook\n",
    "        \"\"\"\n",
    "        import ipywidgets as widgets\n",
    "        \n",
    "        # Create experiment selector\n",
    "        experiment_dropdown = widgets.Dropdown(\n",
    "            options=[(f\"{exp.get('experiment_id')} - {exp.get('model')} - {exp.get('date')}\", \n",
    "                      exp.get('experiment_id')) for exp in discovered_experiments],\n",
    "            description='Experiment:',\n",
    "            disabled=False,\n",
    "        )\n",
    "        \n",
    "        # Display the widget\n",
    "        display(experiment_dropdown)\n",
    "        \n",
    "        # Function to load selected experiment\n",
    "        def load_selected_experiment(change):\n",
    "            global result, metadata\n",
    "            experiment_id = change.new\n",
    "            result, metadata = load_experiment_results(experiment_id)\n",
    "            print(f\"‚úÖ Loaded experiment: {experiment_id}\")\n",
    "            \n",
    "            # Display experiment overview\n",
    "            print(\"\\nüìã Experiment Overview:\")\n",
    "            print(f\"‚Ä¢ Model: {metadata.get('model', 'N/A')}\")\n",
    "            print(f\"‚Ä¢ Date: {metadata.get('date', 'N/A')}\")\n",
    "            print(f\"‚Ä¢ Fields: {', '.join(metadata.get('fields', []))}\")\n",
    "            print(f\"‚Ä¢ Type: {metadata.get('type', 'N/A')}\")\n",
    "        \n",
    "        # Register callback\n",
    "        experiment_dropdown.observe(load_selected_experiment, names='value')\n",
    "        \"\"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading experiment results: {str(e)}\")\n",
    "        print(\"In a real notebook, select a valid experiment ID\")\n",
    "else:\n",
    "    print(\"Utilities not available, cannot analyze experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9480a",
   "metadata": {},
   "source": [
    "## 3. Experiment Performance Analysis\n",
    "\n",
    "Analyze the extraction performance of the selected experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25227f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils_available and 'result' in locals():\n",
    "    try:\n",
    "        # Create a dashboard of experiment results\n",
    "        print(\"üìä Generating Experiment Dashboard...\")\n",
    "        dashboard = viz_manager.show_experiment_results(experiment_id=experiment_id)\n",
    "        \n",
    "        # Overall metrics\n",
    "        print(\"\\nüìà Overall Performance Metrics:\")\n",
    "        if hasattr(result, 'metrics'):\n",
    "            for metric_name, metric_value in result.metrics.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    print(f\"‚Ä¢ {metric_name}: {metric_value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"‚Ä¢ {metric_name}: {metric_value}\")\n",
    "        else:\n",
    "            print(\"No overall metrics available\")\n",
    "        \n",
    "        # Field-specific performance\n",
    "        print(\"\\nüìä Field-Specific Performance:\")\n",
    "        \n",
    "        # Create a performance summary DataFrame\n",
    "        performance_data = []\n",
    "        \n",
    "        if hasattr(result, 'field_results'):\n",
    "            for field, performances in result.field_results.items():\n",
    "                for perf in performances:\n",
    "                    performance_data.append({\n",
    "                        'Field': field,\n",
    "                        'Prompt': perf.prompt_name,\n",
    "                        'Accuracy': perf.accuracy * 100 if hasattr(perf, 'accuracy') else 0,\n",
    "                        'Success Count': perf.successful_extractions if hasattr(perf, 'successful_extractions') else 0,\n",
    "                        'Total Items': perf.total_items if hasattr(perf, 'total_items') else 0\n",
    "                    })\n",
    "        \n",
    "        if performance_data:\n",
    "            performance_df = pd.DataFrame(performance_data)\n",
    "            display(performance_df)\n",
    "            \n",
    "            # Create a bar chart of field accuracies\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='Field', y='Accuracy', hue='Prompt', data=performance_df)\n",
    "            plt.title('Extraction Accuracy by Field')\n",
    "            plt.xlabel('Field')\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No field performance data available\")\n",
    "            \n",
    "        # Sample extractions\n",
    "        print(\"\\nüìù Sample Extractions:\")\n",
    "        if hasattr(result, 'extractions') and result.extractions:\n",
    "            # Create a DataFrame for sample extractions\n",
    "            sample_data = []\n",
    "            for i, extraction in enumerate(result.extractions[:5]):  # Show first 5\n",
    "                row = {'Document ID': extraction.document_id}\n",
    "                \n",
    "                # Add extracted fields\n",
    "                if hasattr(extraction, 'extracted_fields'):\n",
    "                    for field, value in extraction.extracted_fields.items():\n",
    "                        row[field] = value\n",
    "                \n",
    "                # Add accuracy if available\n",
    "                if hasattr(extraction, 'accuracy'):\n",
    "                    row['Accuracy'] = extraction.accuracy\n",
    "                \n",
    "                sample_data.append(row)\n",
    "            \n",
    "            if sample_data:\n",
    "                sample_df = pd.DataFrame(sample_data)\n",
    "                display(sample_df)\n",
    "        else:\n",
    "            print(\"No sample extractions available\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing experiment: {str(e)}\")\n",
    "else:\n",
    "    print(\"Load an experiment first to analyze its performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a571b9",
   "metadata": {},
   "source": [
    "## 4. Compare Experiments\n",
    "\n",
    "Compare multiple experiments to identify the best performing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils_available:\n",
    "    # Initialize experiment comparator\n",
    "    comparator = ExperimentComparator(experiment_loader=experiment_loader)\n",
    "    \n",
    "    # Select experiments to compare\n",
    "    # In a real notebook, users would select these interactively\n",
    "    if len(discovered_experiments) >= 2:\n",
    "        # Take two most recent experiments for demonstration\n",
    "        sorted_experiments = sorted(discovered_experiments, key=lambda x: x.get('date', ''), reverse=True)\n",
    "        experiment_ids = [exp.get('experiment_id') for exp in sorted_experiments[:2]]\n",
    "        \n",
    "        print(f\"Comparing experiments: {', '.join(experiment_ids)}\")\n",
    "        \n",
    "        # Compare experiments\n",
    "        try:\n",
    "            comparison_result = comparator.compare_experiments(\n",
    "                experiment_ids=experiment_ids,\n",
    "                name=\"Example Comparison\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\nüìä Experiment Comparison Results:\")\n",
    "            \n",
    "            # Display comparison data\n",
    "            if hasattr(comparison_result, 'data_points'):\n",
    "                comparison_data = []\n",
    "                \n",
    "                for data_point in comparison_result.data_points:\n",
    "                    comparison_data.append({\n",
    "                        'Experiment': data_point.label,\n",
    "                        'Accuracy': data_point.metrics.get('accuracy', 0) * 100,\n",
    "                        'Processing Time': data_point.metrics.get('processing_time', 0),\n",
    "                        'Sample Size': data_point.sample_size\n",
    "                    })\n",
    "                \n",
    "                if comparison_data:\n",
    "                    comparison_df = pd.DataFrame(comparison_data)\n",
    "                    display(comparison_df)\n",
    "                    \n",
    "                    # Create comparison visualization\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    sns.barplot(x='Experiment', y='Accuracy', data=comparison_df)\n",
    "                    plt.title('Accuracy Comparison Between Experiments')\n",
    "                    plt.xlabel('Experiment')\n",
    "                    plt.ylabel('Accuracy (%)')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            else:\n",
    "                print(\"No comparison data available\")\n",
    "            \n",
    "            # Show statistical significance\n",
    "            if hasattr(comparison_result, 'statistics'):\n",
    "                print(\"\\nüìà Statistical Analysis:\")\n",
    "                \n",
    "                for stat_name, stat_value in comparison_result.statistics.items():\n",
    "                    print(f\"‚Ä¢ {stat_name}: {stat_value}\")\n",
    "                \n",
    "                if 'significant_difference' in comparison_result.statistics:\n",
    "                    sig_diff = comparison_result.statistics['significant_difference']\n",
    "                    if sig_diff:\n",
    "                        print(\"‚úÖ The difference between experiments is statistically significant\")\n",
    "                    else:\n",
    "                        print(\"‚ùå The difference between experiments is NOT statistically significant\")\n",
    "            \n",
    "            # Show visualization suggestions\n",
    "            if hasattr(comparison_result, 'visualization_suggestions') and comparison_result.visualization_suggestions:\n",
    "                print(\"\\nüé® Visualization Suggestions:\")\n",
    "                for suggestion in comparison_result.visualization_suggestions:\n",
    "                    print(f\"‚Ä¢ {suggestion}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error comparing experiments: {str(e)}\")\n",
    "    else:\n",
    "        print(\"At least two experiments are needed for comparison. Run more experiments first.\")\n",
    "        \n",
    "    # Provide code template for model comparison\n",
    "    print(\"\\nüí° To compare different models:\")\n",
    "    print(\"\"\"\n",
    "    # Example:\n",
    "    model_comparison = comparator.compare_models(\n",
    "        model_names=[\"llava-1.5-7b\", \"phi-2\"],\n",
    "        field=\"invoice_number\"\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Provide code template for prompt comparison\n",
    "    print(\"\\nüí° To compare different prompts:\")\n",
    "    print(\"\"\"\n",
    "    # Example:\n",
    "    prompt_comparison = comparator.compare_experiments(\n",
    "        experiment_ids=[\"exp_id_1\", \"exp_id_2\"],\n",
    "        field=\"invoice_number\",\n",
    "        comparison_dimension=\"prompt\"\n",
    "    )\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"Utilities not available, cannot compare experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f824a47",
   "metadata": {},
   "source": [
    "## 5. Advanced Analysis Templates\n",
    "\n",
    "Select from pre-configured analysis templates for common scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c835e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils_available:\n",
    "    print(\"üìä Available Analysis Templates:\")\n",
    "    print(\"1. Model Performance Comparison - Compare accuracy across different models\")\n",
    "    print(\"2. Field Extraction Difficulty - Analyze which fields are hardest to extract\")\n",
    "    print(\"3. Quantization Impact - Measure the impact of model quantization on performance\")\n",
    "    print(\"4. Prompt Effectiveness - Compare different prompt strategies\")\n",
    "    \n",
    "    # In a real notebook, users would select a template interactively\n",
    "    selected_template = 1  # For demonstration\n",
    "    print(f\"\\nSelected template: Model Performance Comparison\")\n",
    "    \n",
    "    # Execute the selected template\n",
    "    if selected_template == 1 and experiment_loader:\n",
    "        # Model Performance Comparison template\n",
    "        print(\"\\nüìä Executing Model Performance Comparison...\")\n",
    "        \n",
    "        # Find experiments with different models\n",
    "        model_experiments = experiment_loader.filter_experiments(\n",
    "            min_date=\"2023-01-01\"  # Filter to recent experiments\n",
    "        )\n",
    "        \n",
    "        # Group by model\n",
    "        model_groups = {}\n",
    "        for exp in model_experiments:\n",
    "            model = exp.get('model', 'Unknown')\n",
    "            if model not in model_groups:\n",
    "                model_groups[model] = []\n",
    "            model_groups[model].append(exp)\n",
    "        \n",
    "        # Use the most recent experiment for each model\n",
    "        most_recent_by_model = {}\n",
    "        for model, exps in model_groups.items():\n",
    "            if exps:\n",
    "                most_recent = sorted(exps, key=lambda x: x.get('date', ''), reverse=True)[0]\n",
    "                most_recent_by_model[model] = most_recent\n",
    "        \n",
    "        if len(most_recent_by_model) >= 2:\n",
    "            # Create model comparison\n",
    "            model_names = list(most_recent_by_model.keys())\n",
    "            print(f\"Comparing models: {', '.join(model_names)}\")\n",
    "            \n",
    "            try:\n",
    "                model_comparison = comparator.compare_models(\n",
    "                    model_names=model_names\n",
    "                )\n",
    "                \n",
    "                # Display comparison results\n",
    "                if hasattr(model_comparison, 'data_points'):\n",
    "                    model_data = []\n",
    "                    for data_point in model_comparison.data_points:\n",
    "                        model_data.append({\n",
    "                            'Model': data_point.label,\n",
    "                            'Accuracy': data_point.metrics.get('accuracy', 0) * 100,\n",
    "                            'Processing Time': data_point.metrics.get('processing_time', 0),\n",
    "                            'Memory Usage': data_point.metrics.get('memory_usage', 0)\n",
    "                        })\n",
    "                    \n",
    "                    if model_data:\n",
    "                        model_df = pd.DataFrame(model_data)\n",
    "                        display(model_df)\n",
    "                        \n",
    "                        # Create visualization\n",
    "                        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "                        \n",
    "                        # Accuracy bars\n",
    "                        sns.barplot(x='Model', y='Accuracy', data=model_df, ax=ax1, color='blue', alpha=0.7)\n",
    "                        ax1.set_ylabel('Accuracy (%)', color='blue')\n",
    "                        \n",
    "                        # Processing time line on secondary axis\n",
    "                        ax2 = ax1.twinx()\n",
    "                        sns.pointplot(x='Model', y='Processing Time', data=model_df, ax=ax2, color='red')\n",
    "                        ax2.set_ylabel('Processing Time (s)', color='red')\n",
    "                        \n",
    "                        plt.title('Model Comparison: Accuracy vs. Processing Time')\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        \n",
    "                        # Recommendations\n",
    "                        print(\"\\nüîç Analysis Recommendations:\")\n",
    "                        best_accuracy_model = model_df.loc[model_df['Accuracy'].idxmax()]['Model']\n",
    "                        fastest_model = model_df.loc[model_df['Processing Time'].idxmin()]['Model']\n",
    "                        \n",
    "                        print(f\"‚Ä¢ Best accuracy: {best_accuracy_model}\")\n",
    "                        print(f\"‚Ä¢ Fastest processing: {fastest_model}\")\n",
    "                        \n",
    "                        if best_accuracy_model == fastest_model:\n",
    "                            print(f\"‚úÖ {best_accuracy_model} offers the best overall performance\")\n",
    "                        else:\n",
    "                            print(f\"üí° Consider {best_accuracy_model} for accuracy-critical tasks\")\n",
    "                            print(f\"üí° Consider {fastest_model} for speed-critical tasks\")\n",
    "                else:\n",
    "                    print(\"No comparison data available\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in model comparison: {str(e)}\")\n",
    "        else:\n",
    "            print(\"Need at least two different models to compare. Run experiments with different models first.\")\n",
    "    \n",
    "    # Provide code for other templates\n",
    "    print(\"\\nüí° To execute other analysis templates, select a different template number.\")\n",
    "else:\n",
    "    print(\"Utilities not available, cannot run analysis templates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9db91",
   "metadata": {},
   "source": [
    "## 6. Export Results\n",
    "\n",
    "Export your analysis for sharing or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils_available and 'result' in locals():\n",
    "    # Export options\n",
    "    print(\"üì§ Export Options:\")\n",
    "    print(\"1. Export to HTML\")\n",
    "    print(\"2. Export to Markdown\")\n",
    "    print(\"3. Export to CSV\")\n",
    "    print(\"4. Export to JSON\")\n",
    "    \n",
    "    # In a real notebook, users would select an option interactively\n",
    "    export_format = 1  # For demonstration\n",
    "    export_path = f\"./export_{experiment_id}.html\"  # Default path\n",
    "    \n",
    "    print(f\"\\nSelected export format: HTML\")\n",
    "    print(f\"Export path: {export_path}\")\n",
    "    \n",
    "    # Export based on selected format\n",
    "    try:\n",
    "        if export_format == 1:  # HTML\n",
    "            # Generate HTML report using the visualization manager\n",
    "            if viz_manager:\n",
    "                html_report = viz_manager.export_experiment_report(\n",
    "                    experiment_id=experiment_id,\n",
    "                    output_format=\"html\"\n",
    "                )\n",
    "                \n",
    "                # Save to file\n",
    "                with open(export_path, 'w') as f:\n",
    "                    f.write(html_report)\n",
    "                print(f\"‚úÖ Exported HTML report to: {export_path}\")\n",
    "        elif export_format == 2:  # Markdown\n",
    "            # Generate Markdown report\n",
    "            if viz_manager:\n",
    "                md_report = viz_manager.export_experiment_report(\n",
    "                    experiment_id=experiment_id,\n",
    "                    output_format=\"markdown\"\n",
    "                )\n",
    "                \n",
    "                # Save to file\n",
    "                md_path = export_path.replace('.html', '.md')\n",
    "                with open(md_path, 'w') as f:\n",
    "                    f.write(md_report)\n",
    "                print(f\"‚úÖ Exported Markdown report to: {md_path}\")\n",
    "        elif export_format == 3:  # CSV\n",
    "            # Export relevant data to CSV\n",
    "            if hasattr(result, 'field_results'):\n",
    "                # Create a flattened DataFrame\n",
    "                export_data = []\n",
    "                \n",
    "                for field, performances in result.field_results.items():\n",
    "                    for perf in performances:\n",
    "                        row = {\n",
    "                            'Field': field,\n",
    "                            'Prompt': perf.prompt_name if hasattr(perf, 'prompt_name') else 'Default',\n",
    "                            'Accuracy': perf.accuracy if hasattr(perf, 'accuracy') else 0,\n",
    "                            'Success_Count': perf.successful_extractions if hasattr(perf, 'successful_extractions') else 0,\n",
    "                            'Total_Items': perf.total_items if hasattr(perf, 'total_items') else 0\n",
    "                        }\n",
    "                        export_data.append(row)\n",
    "                \n",
    "                # Create DataFrame and export\n",
    "                if export_data:\n",
    "                    export_df = pd.DataFrame(export_data)\n",
    "                    csv_path = export_path.replace('.html', '.csv')\n",
    "                    export_df.to_csv(csv_path, index=False)\n",
    "                    print(f\"‚úÖ Exported CSV data to: {csv_path}\")\n",
    "                else:\n",
    "                    print(\"No data available to export\")\n",
    "            else:\n",
    "                print(\"No field results available to export\")\n",
    "        elif export_format == 4:  # JSON\n",
    "            # Export result as JSON\n",
    "            json_path = export_path.replace('.html', '.json')\n",
    "            \n",
    "            # Convert result to JSON-serializable format\n",
    "            if hasattr(result, 'to_dict'):\n",
    "                json_data = result.to_dict()\n",
    "            else:\n",
    "                # Use __dict__ as fallback\n",
    "                json_data = result.__dict__\n",
    "            \n",
    "            with open(json_path, 'w') as f:\n",
    "                json.dump(json_data, f, indent=2, default=str)\n",
    "            print(f\"‚úÖ Exported JSON data to: {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting results: {str(e)}\")\n",
    "    \n",
    "    # Provide code for programmatic export\n",
    "    print(\"\\nüí° To customize export:\")\n",
    "    print(\"\"\"\n",
    "    # Example for custom HTML export:\n",
    "    custom_html = viz_manager.export_experiment_report(\n",
    "        experiment_id=experiment_id,\n",
    "        output_format=\"html\",\n",
    "        include_visualizations=True,\n",
    "        include_raw_data=False\n",
    "    )\n",
    "    \n",
    "    with open(\"custom_export.html\", \"w\") as f:\n",
    "        f.write(custom_html)\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"Load an experiment first to export its results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83e575",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Based on your analysis, you can:\n",
    "\n",
    "1. **Refine your experiments** - Adjust model parameters, prompts, or fields based on results\n",
    "2. **Deploy your best model** - Use the Model Deployment notebook to deploy your best performing model\n",
    "3. **Create custom visualizations** - Use the raw data to create custom visualizations for specific needs\n",
    "4. **Share your results** - Export and share your analysis with team members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b853ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(\"To analyze different experiments, return to Step 2 and select a different experiment.\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
