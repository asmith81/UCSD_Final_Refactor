{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0f14da",
   "metadata": {},
   "source": [
    "# Invoice Extraction System: Environment Setup\n",
    "\n",
    "This notebook guides you through the process of setting up and validating your environment for invoice extraction experiments. By the end of this notebook, you'll have:\n",
    "\n",
    "1. A properly configured environment with all necessary dependencies\n",
    "2. Validated GPU setup with appropriate memory settings\n",
    "3. Configured paths for data, models, and results\n",
    "4. Verified model availability and compatibility\n",
    "5. Tested the system with a simple extraction task\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83031657",
   "metadata": {},
   "source": [
    "## 1. Environment Detection\n",
    "\n",
    "First, let's detect your current environment and display key system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the project root to the path if not already there\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import our utility modules\n",
    "try:\n",
    "    from src.notebook.setup_utils import (\n",
    "        validate_environment, \n",
    "        check_gpu_availability, \n",
    "        configure_paths, \n",
    "        get_system_info\n",
    "    )\n",
    "    from src.notebook.error_utils import display_error, NotebookFriendlyError, safe_execute\n",
    "    setup_utils_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Error importing setup utilities: {str(e)}\")\n",
    "    print(\"‚ö†Ô∏è Will use basic environment detection instead.\")\n",
    "    setup_utils_available = False\n",
    "\n",
    "# Basic environment information\n",
    "print(f\"üìã Environment Setup and Validation\")\n",
    "print(f\"üïí Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üíª System: {platform.system()} {platform.release()}\")\n",
    "print(f\"üêç Python version: {platform.python_version()}\")\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in a notebook environment\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    in_notebook = get_ipython().__class__.__name__ == 'ZMQInteractiveShell'\n",
    "    print(f\"üìì Running in notebook environment: {in_notebook}\")\n",
    "except (ImportError, NameError, AttributeError):\n",
    "    in_notebook = False\n",
    "    print(f\"üìì Running in script environment\")\n",
    "\n",
    "# Use our utility module if available\n",
    "if setup_utils_available:\n",
    "    system_info = get_system_info()\n",
    "    print(f\"\\nüìä Detailed System Information:\")\n",
    "    print(f\"üîç Platform: {system_info['platform']}\")\n",
    "    print(f\"üß† Python implementation: {system_info['python_implementation']}\")\n",
    "    print(f\"üìÇ Python path: {system_info['python_path']}\")\n",
    "    \n",
    "    # Check if we're in a RunPod environment\n",
    "    if system_info['in_runpod']:\n",
    "        print(f\"‚òÅÔ∏è RunPod environment detected!\")\n",
    "        print(f\"üÜî Pod ID: {system_info['env_vars'].get('RUNPOD_POD_ID', 'Unknown')}\")\n",
    "        print(f\"üíæ Disk space: {system_info['disk_space']['free_gb']:.1f} GB free of {system_info['disk_space']['total_gb']:.1f} GB\")\n",
    "    else:\n",
    "        print(f\"üíª Local environment detected\")\n",
    "else:\n",
    "    # Fallback if our utility module isn't available\n",
    "    def is_runpod():\n",
    "        \"\"\"Check if we're running in a RunPod environment\"\"\"\n",
    "        return (\n",
    "            os.environ.get(\"RUNPOD_POD_ID\") is not None or \n",
    "            \"A100\" in os.environ.get(\"GPU_NAME\", \"\") or\n",
    "            \"H100\" in os.environ.get(\"GPU_NAME\", \"\")\n",
    "        )\n",
    "    \n",
    "    if is_runpod():\n",
    "        print(f\"‚òÅÔ∏è RunPod environment detected!\")\n",
    "    else:\n",
    "        print(f\"üíª Local environment detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c416658",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration and Validation\n",
    "\n",
    "Now, let's check for GPU availability and configuration. This is crucial for efficient model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94447baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Use our utility module if available\n",
    "if setup_utils_available:\n",
    "    gpu_info = check_gpu_availability()\n",
    "    \n",
    "    if gpu_info['available']:\n",
    "        print(f\"‚úÖ GPU detected: {gpu_info['name']}\")\n",
    "        print(f\"üìä GPU memory: {gpu_info['memory_total_gb']:.2f} GB total, {gpu_info['memory_free_gb']:.2f} GB free\")\n",
    "        print(f\"üî¢ CUDA version: {gpu_info['cuda_version']}\")\n",
    "        print(f\"üîß Driver version: {gpu_info['driver_version']}\")\n",
    "        \n",
    "        # Memory recommendations based on GPU\n",
    "        if gpu_info['memory_total_gb'] < 16:\n",
    "            print(f\"‚ö†Ô∏è Limited GPU memory detected. Will need to use quantization for larger models.\")\n",
    "            print(f\"   Recommended model: 'phi-2' or other smaller models with quantization\")\n",
    "        elif gpu_info['memory_total_gb'] < 40:\n",
    "            print(f\"‚úÖ Sufficient GPU memory for mid-sized models.\")\n",
    "            print(f\"   Recommended models: Llama-7B, Mistral-7B with 8-bit quantization\")\n",
    "        else:\n",
    "            print(f\"üöÄ Excellent GPU memory. Can run large models without quantization.\")\n",
    "            print(f\"   Recommended models: Llama-13B, Mixtral-8x7B, or Llama2-70B with quantization\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No GPU detected. Using CPU mode, but processing will be significantly slower.\")\n",
    "        print(f\"   Consider using a smaller model like 'phi-2' for CPU inference.\")\n",
    "else:\n",
    "    # Fallback GPU detection\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"‚úÖ GPU detected: {device_name}\")\n",
    "            print(f\"üìä GPU memory: {memory:.2f} GB\")\n",
    "            print(f\"üî¢ CUDA version: {torch.version.cuda}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No GPU detected. Using CPU mode, but processing will be significantly slower.\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ö†Ô∏è PyTorch not installed, cannot check GPU availability.\")\n",
    "        try:\n",
    "            gpu_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "            print(f\"‚úÖ GPU detected via nvidia-smi:\")\n",
    "            for line in gpu_info.split('\\n')[:10]:  # Show first 10 lines\n",
    "                if \"NVIDIA\" in line and not \"Driver\" in line:\n",
    "                    print(f\"   {line.strip()}\")\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è No GPU detected via nvidia-smi. Using CPU mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588fe63",
   "metadata": {},
   "source": [
    "## 3. Dependency Installation and Verification\n",
    "\n",
    "Let's install and verify the required dependencies for invoice extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f725a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "\n",
    "# Function to check if a package is installed\n",
    "def is_package_installed(package_name):\n",
    "    return importlib.util.find_spec(package_name) is not None\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch\", \"torchvision\", \"transformers\", \"pillow\", \n",
    "    \"pandas\", \"numpy\", \"matplotlib\", \"pyyaml\", \"tqdm\"\n",
    "]\n",
    "\n",
    "# Check which packages are installed\n",
    "print(\"üì¶ Checking required packages:\")\n",
    "missing_packages = []\n",
    "for package in required_packages:\n",
    "    if is_package_installed(package):\n",
    "        # Get version if possible\n",
    "        try:\n",
    "            module = importlib.import_module(package)\n",
    "            version = getattr(module, \"__version__\", \"unknown version\")\n",
    "            print(f\"‚úÖ {package} ({version})\")\n",
    "        except:\n",
    "            print(f\"‚úÖ {package}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {package} - not installed\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "# Install missing packages if needed\n",
    "if missing_packages:\n",
    "    print(f\"\\nüì• Installing {len(missing_packages)} missing packages...\")\n",
    "    for package in missing_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ Installed {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package}: {str(e)}\")\n",
    "\n",
    "# Special check for transformers\n",
    "if is_package_installed(\"transformers\"):\n",
    "    try:\n",
    "        import transformers\n",
    "        print(f\"\\nü§ó Transformers version: {transformers.__version__}\")\n",
    "        \n",
    "        # Check if we can load a basic model\n",
    "        print(f\"üß™ Testing transformers model loading capabilities...\")\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=\"models/cache/bert-test\")\n",
    "        print(f\"‚úÖ Successfully loaded test tokenizer\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading transformers model: {str(e)}\")\n",
    "\n",
    "# Check PyTorch with CUDA\n",
    "if is_package_installed(\"torch\"):\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"\\nüî• PyTorch version: {torch.__version__}\")\n",
    "        print(f\"üîç CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üñ•Ô∏è CUDA device count: {torch.cuda.device_count()}\")\n",
    "            print(f\"üñ•Ô∏è Current CUDA device: {torch.cuda.current_device()}\")\n",
    "            print(f\"üñ•Ô∏è CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "            \n",
    "            # Test a basic CUDA operation\n",
    "            print(f\"üß™ Testing CUDA tensor operations...\")\n",
    "            a = torch.tensor([1.0, 2.0, 3.0], device=\"cuda\")\n",
    "            b = torch.tensor([4.0, 5.0, 6.0], device=\"cuda\")\n",
    "            c = a + b\n",
    "            print(f\"‚úÖ CUDA tensor operation successful: {c.cpu().numpy()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error testing PyTorch CUDA: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb42ac",
   "metadata": {},
   "source": [
    "## 4. Path Configuration\n",
    "\n",
    "Let's set up and validate the paths for data, models, results, and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use our utility module if available\n",
    "if setup_utils_available:\n",
    "    paths = configure_paths()\n",
    "    \n",
    "    print(f\"üìÇ Path Configuration:\")\n",
    "    for key, path in paths.items():\n",
    "        status = \"‚úÖ\" if os.path.exists(path) else \"‚ùå\"\n",
    "        print(f\"{status} {key}: {path}\")\n",
    "        \n",
    "    # Set environment variables for paths\n",
    "    for key, path in paths.items():\n",
    "        env_var = key.upper()\n",
    "        os.environ[env_var] = str(path)\n",
    "        print(f\"üîÑ Set environment variable {env_var}={path}\")\n",
    "else:\n",
    "    # Fallback path configuration\n",
    "    # Detect project root\n",
    "    current_dir = Path.cwd()\n",
    "    \n",
    "    # Search for project root by looking for standard directories\n",
    "    project_root = current_dir\n",
    "    while project_root != project_root.parent and not all(\n",
    "        (project_root / d).exists() for d in [\"src\", \"data\", \"models\"]\n",
    "    ):\n",
    "        project_root = project_root.parent\n",
    "    \n",
    "    if project_root == project_root.parent:\n",
    "        # If we reached the file system root without finding our markers\n",
    "        # Use the current directory as project root\n",
    "        project_root = current_dir\n",
    "        print(\"‚ö†Ô∏è Could not detect project root. Using current directory.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found project root: {project_root}\")\n",
    "    \n",
    "    # Define key paths\n",
    "    paths = {\n",
    "        \"project_root\": project_root,\n",
    "        \"data_dir\": project_root / \"data\",\n",
    "        \"models_dir\": project_root / \"models\",\n",
    "        \"results_dir\": project_root / \"results\",\n",
    "        \"logs_dir\": project_root / \"logs\",\n",
    "        \"config_dir\": project_root / \"configs\",\n",
    "    }\n",
    "    \n",
    "    # Check if paths exist\n",
    "    print(f\"\\nüìÇ Path Configuration:\")\n",
    "    for key, path in paths.items():\n",
    "        os.makedirs(path, exist_ok=True)  # Create if doesn't exist\n",
    "        status = \"‚úÖ\" if os.path.exists(path) else \"‚ùå\"\n",
    "        print(f\"{status} {key}: {path}\")\n",
    "        \n",
    "        # Set environment variables for paths\n",
    "        env_var = key.upper()\n",
    "        os.environ[env_var] = str(path)\n",
    "        print(f\"üîÑ Set environment variable {env_var}={path}\")\n",
    "\n",
    "# Check if data directory has any invoice images\n",
    "data_path = Path(os.environ.get(\"DATA_DIR\", \"data\"))\n",
    "image_files = list(data_path.glob(\"**/*.png\")) + list(data_path.glob(\"**/*.jpg\")) + list(data_path.glob(\"**/*.jpeg\"))\n",
    "\n",
    "if image_files:\n",
    "    print(f\"‚úÖ Found {len(image_files)} image files in data directory\")\n",
    "    if len(image_files) > 5:\n",
    "        print(f\"   Sample files: {', '.join([f.name for f in image_files[:5]])}\")\n",
    "    else:\n",
    "        print(f\"   Files: {', '.join([f.name for f in image_files])}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No image files found in data directory. Will need to add data before extraction.\")\n",
    "\n",
    "# Check for model files\n",
    "models_path = Path(os.environ.get(\"MODELS_DIR\", \"models\"))\n",
    "model_dirs = [d for d in models_path.glob(\"*\") if d.is_dir()]\n",
    "\n",
    "if model_dirs:\n",
    "    print(f\"‚úÖ Found {len(model_dirs)} potential model directories\")\n",
    "    for model_dir in model_dirs:\n",
    "        num_files = len(list(model_dir.glob(\"*\")))\n",
    "        print(f\"   {model_dir.name}: {num_files} files\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No model directories found. Models will be downloaded on first use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37476b63",
   "metadata": {},
   "source": [
    "## 5. Model Availability Check\n",
    "\n",
    "The invoice extraction system is designed to download and use models on demand. Let's check what models are currently available locally and explain how the model selection system works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üîç Model Management System\")\n",
    "print(\"==========================\")\n",
    "print(\"The extraction system supports various vision-language models for invoice processing.\")\n",
    "print(\"Models are downloaded automatically when first used, so you don't need to pre-download them.\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Try to use our experiment utils module to list any locally available models\n",
    "    from src.notebook.experiment_utils import list_available_models\n",
    "    \n",
    "    print(\"üì¶ Checking for locally available models...\")\n",
    "    models = list_available_models()\n",
    "    \n",
    "    if models:\n",
    "        print(f\"‚úÖ Found {len(models)} already downloaded models:\")\n",
    "        for model in models:\n",
    "            print(f\"   üìã {model['name']} ({model['size']:.2f} GB)\")\n",
    "            if 'architecture' in model and model['architecture'] != \"Unknown\":\n",
    "                print(f\"      Architecture: {model['architecture']}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No pre-downloaded models found locally.\")\n",
    "        print(\"   This is normal - models will be downloaded automatically when you run your first experiment.\")\n",
    "        \n",
    "    # List recommended models\n",
    "    print(\"\\nüåü Recommended Models\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Try to import the model registry if available\n",
    "    try:\n",
    "        from src.models.registry import get_available_model_configs\n",
    "        \n",
    "        print(\"These models are configured and ready to use in your experiments:\")\n",
    "        model_configs = get_available_model_configs()\n",
    "        for model_name, config in model_configs.items():\n",
    "            memory_req = config.get(\"memory_requirements\", {}).get(\"gpu_gb\", \"Unknown\")\n",
    "            model_type = config.get(\"model_type\", \"Unknown\")\n",
    "            specialist = \"‚úì\" if config.get(\"is_invoice_specialist\", False) else \" \"\n",
    "            print(f\"   ‚Ä¢ {model_name}: {model_type} model, ~{memory_req} GB GPU required, Invoice Specialist: {specialist}\")\n",
    "    except ImportError:\n",
    "        # Fallback recommended models\n",
    "        print(\"Common models you can use include:\")\n",
    "        print(\"   ‚Ä¢ 'phi-2': Smaller model (2.7B params), works on limited GPU memory or CPU\")\n",
    "        print(\"   ‚Ä¢ 'llava-1.5-7b': Mid-sized multimodal model (7B params), good balance of performance and speed\")\n",
    "        print(\"   ‚Ä¢ 'llava-1.5-13b': Larger multimodal model (13B params), better performance but requires more GPU memory\")\n",
    "        print(\"   ‚Ä¢ 'bakllava-1': Mixture of experts model based on Mixtral, excellent performance but high memory requirements\")\n",
    "        \n",
    "    print(\"\\n‚öôÔ∏è How Model Selection Works\")\n",
    "    print(\"-------------------------\")\n",
    "    print(\"1. When you create an experiment, you specify which model to use\")\n",
    "    print(\"2. If the model isn't already downloaded, it will be automatically downloaded\")\n",
    "    print(\"3. The model is loaded with appropriate optimizations based on your hardware\")\n",
    "    print(\"4. You can use quantization to reduce memory requirements for larger models\")\n",
    "    \n",
    "    # Memory requirements guidance\n",
    "    print(\"\\nüíæ Memory Requirements Guide\")\n",
    "    print(\"-------------------------\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"Your GPU has {gpu_mem:.1f} GB of memory. Based on this:\")\n",
    "            \n",
    "            if gpu_mem < 8:\n",
    "                print(\"   ‚Ä¢ Recommended: Use smaller models like 'phi-2' with 4-bit quantization\")\n",
    "                print(\"   ‚Ä¢ Not recommended: Models larger than 7B parameters\")\n",
    "            elif gpu_mem < 16:\n",
    "                print(\"   ‚Ä¢ Recommended: 7B parameter models with 8-bit quantization\")\n",
    "                print(\"   ‚Ä¢ Possible with care: 13B models with 4-bit quantization\")\n",
    "                print(\"   ‚Ä¢ Not recommended: Models larger than 13B parameters\")\n",
    "            elif gpu_mem < 32:\n",
    "                print(\"   ‚Ä¢ Recommended: 13B parameter models with 8-bit quantization\")\n",
    "                print(\"   ‚Ä¢ Possible with care: Mixture-of-experts models with 4-bit quantization\")\n",
    "            else:\n",
    "                print(\"   ‚Ä¢ Your GPU can handle most models, including large mixture-of-experts models\")\n",
    "                print(\"   ‚Ä¢ For best performance, still consider 8-bit quantization for the largest models\")\n",
    "        else:\n",
    "            print(\"No GPU detected. When using CPU:\")\n",
    "            print(\"   ‚Ä¢ Recommended: Use smaller models like 'phi-2'\")\n",
    "            print(\"   ‚Ä¢ Processing will be significantly slower than with a GPU\")\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not installed, cannot provide specific memory guidance.\")\n",
    "except ImportError:\n",
    "    # Fallback basic model discovery\n",
    "    models_dir = os.environ.get(\"MODELS_DIR\", \"models\")\n",
    "    if os.path.exists(models_dir):\n",
    "        model_dirs = [d for d in os.listdir(models_dir) \n",
    "                      if os.path.isdir(os.path.join(models_dir, d))]\n",
    "        if model_dirs:\n",
    "            print(f\"‚úÖ Found {len(model_dirs)} potential model directories:\")\n",
    "            for model_dir in model_dirs:\n",
    "                print(f\"   üì¶ {model_dir}\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No model directories found - models will be downloaded when needed.\")\n",
    "            \n",
    "        print(\"\\nCommon models you can use include:\")\n",
    "        print(\"   ‚Ä¢ 'phi-2': Smaller model, works on limited GPU memory\")\n",
    "        print(\"   ‚Ä¢ 'llava-1.5-7b': Mid-sized multimodal model, good balance\")\n",
    "        print(\"   ‚Ä¢ 'llava-1.5-13b': Larger model, better performance\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Models directory not found.\")\n",
    "        print(\"A models directory will be created when you run your first experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc590ce9",
   "metadata": {},
   "source": [
    "## 6. Data Validation\n",
    "\n",
    "Let's validate that we have data available for extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14097d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# Find invoice images and ground truth files\n",
    "data_dir = os.environ.get(\"DATA_DIR\", \"data\")\n",
    "image_extensions = ['.png', '.jpg', '.jpeg']\n",
    "\n",
    "# Search for invoice images\n",
    "invoice_images = []\n",
    "for ext in image_extensions:\n",
    "    invoice_images.extend(Path(data_dir).glob(f\"**/*{ext}\"))\n",
    "\n",
    "# Check if we found images\n",
    "if invoice_images:\n",
    "    print(f\"‚úÖ Found {len(invoice_images)} invoice images\")\n",
    "    \n",
    "    # Show sample of images if many are found\n",
    "    if len(invoice_images) > 5:\n",
    "        sample_size = min(5, len(invoice_images))\n",
    "        print(f\"\\nüìä Sample of {sample_size} images:\")\n",
    "        for i, img_path in enumerate(invoice_images[:sample_size]):\n",
    "            relative_path = os.path.relpath(img_path, data_dir)\n",
    "            img_size_kb = os.path.getsize(img_path) / 1024\n",
    "            print(f\"   {i+1}. {relative_path} - {img_size_kb:.1f} KB\")\n",
    "    else:\n",
    "        print(\"\\nüìä Available images:\")\n",
    "        for i, img_path in enumerate(invoice_images):\n",
    "            relative_path = os.path.relpath(img_path, data_dir)\n",
    "            img_size_kb = os.path.getsize(img_path) / 1024\n",
    "            print(f\"   {i+1}. {relative_path} - {img_size_kb:.1f} KB\")\n",
    "    \n",
    "    # Check for ground truth data\n",
    "    gt_files = list(Path(data_dir).glob(\"**/*.json\")) + list(Path(data_dir).glob(\"**/*.csv\"))\n",
    "    if gt_files:\n",
    "        print(f\"\\n‚úÖ Found {len(gt_files)} potential ground truth files:\")\n",
    "        for i, gt_path in enumerate(gt_files[:3]):  # Show first 3\n",
    "            relative_path = os.path.relpath(gt_path, data_dir)\n",
    "            print(f\"   {i+1}. {relative_path}\")\n",
    "        if len(gt_files) > 3:\n",
    "            print(f\"   ... and {len(gt_files) - 3} more\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No ground truth files found. Evaluation will not be possible.\")\n",
    "else:\n",
    "    print(f\"‚ùå No invoice images found in {data_dir} directory.\")\n",
    "    print(\"   Please add some invoice images before running extraction experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d8a5a",
   "metadata": {},
   "source": [
    "## 7. System Readiness Test\n",
    "\n",
    "Let's run a simple end-to-end test to verify system readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12eda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üß™ Running system readiness test...\")\n",
    "\n",
    "tests = [\n",
    "    (\"Directory Structure\", lambda: all(os.path.exists(p) for p in [\n",
    "        os.environ.get(\"PROJECT_ROOT\", \"\"), \n",
    "        os.environ.get(\"DATA_DIR\", \"data\"),\n",
    "        os.environ.get(\"MODELS_DIR\", \"models\"),\n",
    "        os.environ.get(\"RESULTS_DIR\", \"results\"),\n",
    "        os.environ.get(\"LOGS_DIR\", \"logs\")\n",
    "    ])),\n",
    "    \n",
    "    (\"Python Environment\", lambda: sys.version_info >= (3, 8)),\n",
    "    \n",
    "    (\"Core Dependencies\", lambda: all(importlib.util.find_spec(pkg) is not None \n",
    "                                      for pkg in [\"torch\", \"transformers\", \"pandas\"])),\n",
    "]\n",
    "\n",
    "# Add GPU test if applicable\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        tests.append((\"GPU Functionality\", lambda: torch.tensor([1.0], device=\"cuda\").item() == 1.0))\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Run and report test results\n",
    "all_passed = True\n",
    "for test_name, test_func in tests:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = test_func()\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if result:\n",
    "            print(f\"‚úÖ {test_name} - Passed ({duration:.2f}s)\")\n",
    "        else:\n",
    "            print(f\"‚ùå {test_name} - Failed ({duration:.2f}s)\")\n",
    "            all_passed = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {test_name} - Error: {str(e)}\")\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\nüéâ All system readiness tests passed! Your environment is ready for invoice extraction.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some system readiness tests failed. Please address the issues before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa63f43",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting Guide\n",
    "\n",
    "If you encountered issues with the setup, here are some common problems and solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0f6aa",
   "metadata": {},
   "source": [
    "### Common Issues\n",
    "\n",
    "#### GPU Not Detected\n",
    "- **Issue**: System doesn't detect your GPU or shows \"CUDA not available\"\n",
    "- **Solutions**:\n",
    "  - Ensure you have a compatible NVIDIA GPU\n",
    "  - Verify NVIDIA drivers are properly installed (`nvidia-smi` should work)\n",
    "  - Check that CUDA is installed and matches your PyTorch version\n",
    "  - Try reinstalling PyTorch with the correct CUDA version: `pip install torch==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118`\n",
    "\n",
    "#### Model Loading Errors\n",
    "- **Issue**: Errors when loading models like \"file not found\" or out of memory\n",
    "- **Solutions**:\n",
    "  - Check your internet connection for downloading models\n",
    "  - Ensure you have enough disk space (models can be several GB)\n",
    "  - For CUDA out of memory errors, try a smaller model or enable quantization\n",
    "  - Verify model compatibility with your environment\n",
    "\n",
    "#### Path Configuration Problems\n",
    "- **Issue**: Files not found or incorrect paths\n",
    "- **Solutions**:\n",
    "  - Ensure you're running from the project root directory\n",
    "  - Check that all required directories exist\n",
    "  - Verify environment variables are correctly set\n",
    "  - If using Windows, check for path format issues\n",
    "\n",
    "#### RunPod-Specific Issues\n",
    "- **Issue**: Problems specific to the RunPod environment\n",
    "- **Solutions**:\n",
    "  - Ensure your pod has enough GPU memory for your selected model\n",
    "  - Check disk space in the mounted volume\n",
    "  - Verify that required dependencies are installed in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d212ae5",
   "metadata": {},
   "source": [
    "## 9. Configuration Summary and Export\n",
    "\n",
    "Let's create a summary of your validated configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Collect configuration summary\n",
    "config_summary = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"environment\": {\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"in_notebook\": in_notebook\n",
    "    },\n",
    "    \"paths\": {\n",
    "        key: str(path) for key, path in paths.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add GPU information if available\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        config_summary[\"gpu\"] = {\n",
    "            \"device_name\": torch.cuda.get_device_name(0),\n",
    "            \"cuda_version\": torch.version.cuda,\n",
    "            \"memory_total_gb\": torch.cuda.get_device_properties(0).total_memory / (1024**3),\n",
    "            \"device_count\": torch.cuda.device_count()\n",
    "        }\n",
    "    else:\n",
    "        config_summary[\"gpu\"] = {\"available\": False}\n",
    "except ImportError:\n",
    "    config_summary[\"gpu\"] = {\"available\": \"unknown (torch not installed)\"}\n",
    "\n",
    "# Add detected data summary\n",
    "config_summary[\"data\"] = {\n",
    "    \"image_count\": len(invoice_images) if 'invoice_images' in locals() else 0,\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"üïí Generated: {config_summary['timestamp']}\")\n",
    "print(f\"üíª Platform: {config_summary['environment']['platform']}\")\n",
    "print(f\"üêç Python: {config_summary['environment']['python_version']}\")\n",
    "print(f\"üìÇ Project root: {config_summary['paths']['project_root']}\")\n",
    "\n",
    "if config_summary['gpu'].get('available', False) not in [False, \"unknown (torch not installed)\"]:\n",
    "    print(f\"üñ•Ô∏è GPU: {config_summary['gpu']['device_name']}\")\n",
    "    print(f\"   CUDA: {config_summary['gpu']['cuda_version']}\")\n",
    "    print(f\"   Memory: {config_summary['gpu']['memory_total_gb']:.2f} GB\")\n",
    "else:\n",
    "    print(\"üñ•Ô∏è GPU: Not available\")\n",
    "\n",
    "print(f\"üñºÔ∏è Invoice images: {config_summary['data']['image_count']}\")\n",
    "\n",
    "# Save configuration summary\n",
    "os.makedirs(os.environ.get(\"LOGS_DIR\", \"logs\"), exist_ok=True)\n",
    "summary_filename = os.path.join(\n",
    "    os.environ.get(\"LOGS_DIR\", \"logs\"), \n",
    "    f\"environment_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    ")\n",
    "\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(config_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration summary saved to: {summary_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e544f",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Congratulations! Your environment is now set up and ready for invoice extraction experiments.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "1. Move on to the **Experiment Configuration Notebook** (02_experiment_configuration.ipynb) to:\n",
    "   - Configure extraction experiments\n",
    "   - Select models and prompts\n",
    "   - Run your first extraction pipeline\n",
    "\n",
    "2. Check out the **Results Analysis Notebook** (03_results_analysis.ipynb) to:\n",
    "   - Analyze extraction results\n",
    "   - Compare different models and prompts\n",
    "   - Visualize extraction performance\n",
    "\n",
    "3. For more in-depth information, explore the documentation in the `docs/` directory:\n",
    "   - Setup guide for additional configuration options\n",
    "   - Architecture documentation for system understanding\n",
    "   - Interface documentation for advanced usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73add49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(\"üöÄ You're ready to proceed to the experiment configuration notebook.\")\n",
    "\n",
    "# Optional: For truly confirming readiness, you could run a minimal extraction test here "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
