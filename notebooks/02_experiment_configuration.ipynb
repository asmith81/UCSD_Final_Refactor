{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ac2d41",
   "metadata": {},
   "source": [
    "# Invoice Extraction: Quick Experiment Execution\n",
    "\n",
    "This notebook allows you to:\n",
    "1. Select an experiment template or create a custom experiment\n",
    "2. Configure model, fields, prompts, and quantization settings\n",
    "3. Run the extraction pipeline\n",
    "4. View and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"invoice_extraction\")\n",
    "\n",
    "# Add the project root to the path if not already there\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import our utility modules\n",
    "try:\n",
    "    from src.notebook.setup_utils import get_system_info, check_gpu_availability\n",
    "    from src.notebook.experiment_utils import (\n",
    "        list_available_models, \n",
    "        list_available_prompts,\n",
    "        list_available_templates,\n",
    "        create_basic_experiment,\n",
    "        create_model_comparison_experiment,\n",
    "        create_prompt_comparison_experiment,\n",
    "        create_quantization_experiment,\n",
    "        load_experiment_template,\n",
    "        run_extraction_experiment,\n",
    "        get_default_fields,\n",
    "        visualize_experiment_results\n",
    "    )\n",
    "    from src.notebook.error_utils import display_error, NotebookFriendlyError\n",
    "    utils_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Error importing utilities: {str(e)}\")\n",
    "    print(\"⚠️ Make sure you've run the environment setup notebook first.\")\n",
    "    utils_available = False\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f\"✅ GPU detected: {device_name} with {gpu_memory:.2f} GB memory\")\n",
    "    else:\n",
    "        print(\"ℹ️ No GPU detected - will use CPU for extraction (slower)\")\n",
    "        gpu_memory = 0\n",
    "except ImportError:\n",
    "    print(\"⚠️ PyTorch not installed - cannot check GPU availability\")\n",
    "    gpu_available = False\n",
    "    gpu_memory = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34509bb5",
   "metadata": {},
   "source": [
    "## 1. Select Experiment Type\n",
    "\n",
    "Choose from available templates or create a custom experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils_available:\n",
    "    # List available templates\n",
    "    templates = list_available_templates()\n",
    "    \n",
    "    print(\"📋 Available experiment templates:\")\n",
    "    if templates:\n",
    "        for i, template in enumerate(templates, 1):\n",
    "            print(f\"{i}. {template['name']}: {template['description']}\")\n",
    "    else:\n",
    "        print(\"No pre-configured templates found. You can create a custom experiment.\")\n",
    "    \n",
    "    # List available experiment types\n",
    "    print(\"\\n📋 Available experiment types:\")\n",
    "    print(\"1. Basic extraction - Test one model, prompt set, and field set\")\n",
    "    print(\"2. Model comparison - Compare multiple models on the same extraction task\")\n",
    "    print(\"3. Prompt comparison - Compare different prompt variants for extraction\")\n",
    "    print(\"4. Quantization comparison - Compare model optimization techniques\")\n",
    "    \n",
    "    # Select experiment type (in a real notebook, this would be interactive)\n",
    "    # For now, we'll default to basic extraction\n",
    "    experiment_type = \"basic\"\n",
    "    print(f\"\\n✅ Selected experiment type: {experiment_type}\")\n",
    "    \n",
    "    # In a real notebook, you would use ipywidgets for selection, something like:\n",
    "    \"\"\"\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # Template selection widget\n",
    "    template_dropdown = widgets.Dropdown(\n",
    "        options=[t['name'] for t in templates] + ['Custom'],\n",
    "        description='Template:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    # Experiment type selection widget\n",
    "    type_dropdown = widgets.Dropdown(\n",
    "        options=['basic', 'model_comparison', 'prompt_comparison', 'quantization_comparison'],\n",
    "        description='Type:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    display(template_dropdown)\n",
    "    display(type_dropdown)\n",
    "    \"\"\"\n",
    "else:\n",
    "    print(\"Utilities not available, cannot list templates or experiment types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006d0cd",
   "metadata": {},
   "source": [
    "## 2. Configure Variables\n",
    "\n",
    "Set up the experiment parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19427264",
   "metadata": {},
   "source": [
    "### Available Configuration Options:\n",
    "\n",
    "1. **Models**: The vision-language model for extraction (e.g., `llava-1.5-7b`, `phi-2`)\n",
    "2. **Fields**: What to extract (e.g., `invoice_number`, `total_amount`)\n",
    "3. **Prompts**: Instructions for the model (can use templates or custom prompts)\n",
    "4. **Quantization**: Memory optimization (e.g., `8bit`, `4bit`, or `none`)\n",
    "5. **Batch Size**: Number of invoices to process at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils_available:\n",
    "    # Display available models\n",
    "    print(\"📦 Available models:\")\n",
    "    models = list_available_models()\n",
    "    available_models = []\n",
    "    \n",
    "    if models:\n",
    "        for model in models:\n",
    "            available_models.append(model['name'])\n",
    "            print(f\"• {model['name']} ({model['size']:.2f} GB)\")\n",
    "    else:\n",
    "        print(\"Models will be downloaded on first use. Common models:\")\n",
    "        available_models = [\"phi-2\", \"llava-1.5-7b\", \"llava-1.5-13b\", \"bakllava-1\"]\n",
    "        for model in available_models:\n",
    "            print(f\"• {model}\")\n",
    "    \n",
    "    # Display available fields\n",
    "    print(\"\\n📝 Available fields:\")\n",
    "    fields = get_default_fields()\n",
    "    for field, description in fields.items():\n",
    "        print(f\"• {field}: {description}\")\n",
    "    \n",
    "    # Display available prompts\n",
    "    print(\"\\n💬 Available prompts:\")\n",
    "    prompts = list_available_prompts()\n",
    "    if prompts:\n",
    "        for field_type, prompt_list in prompts.items():\n",
    "            print(f\"• {field_type}: {', '.join(prompt_list)}\")\n",
    "    else:\n",
    "        print(\"Default prompts will be used\")\n",
    "    \n",
    "    # Display quantization options\n",
    "    print(\"\\n⚙️ Quantization options:\")\n",
    "    print(\"• float32: Full precision (highest accuracy, highest memory usage)\")\n",
    "    print(\"• none: Default precision for the model (usually float16)\")\n",
    "    print(\"• bfloat16: BFloat16 precision (~50% memory saving, good accuracy)\")\n",
    "    print(\"• 8bit: 8-bit quantization (~50% memory saving)\")\n",
    "    print(\"• 4bit: 4-bit quantization (~75% memory saving)\")\n",
    "    \n",
    "    # Recommend configuration based on hardware\n",
    "    print(\"\\n🔍 Recommended configuration for your hardware:\")\n",
    "    if gpu_available:\n",
    "        # Check for bfloat16 support (Ampere or newer GPUs)\n",
    "        has_bfloat16_support = False\n",
    "        try:\n",
    "            has_bfloat16_support = torch.cuda.get_device_capability()[0] >= 8\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if gpu_memory < 8:\n",
    "            recommended_model = \"phi-2\"\n",
    "            recommended_quant = \"4bit\"\n",
    "        elif gpu_memory < 16:\n",
    "            recommended_model = \"llava-1.5-7b\"\n",
    "            if has_bfloat16_support:\n",
    "                recommended_quant = \"bfloat16\"\n",
    "            else:\n",
    "                recommended_quant = \"8bit\"\n",
    "        elif gpu_memory < 32:\n",
    "            recommended_model = \"llava-1.5-13b\"\n",
    "            if has_bfloat16_support:\n",
    "                recommended_quant = \"bfloat16\"\n",
    "            else:\n",
    "                recommended_quant = \"8bit\"\n",
    "        else:\n",
    "            recommended_model = \"bakllava-1\"\n",
    "            # For large GPUs, can consider full precision\n",
    "            if gpu_memory > 48:\n",
    "                recommended_quant = \"float32\"\n",
    "            elif has_bfloat16_support:\n",
    "                recommended_quant = \"bfloat16\"\n",
    "            else:\n",
    "                recommended_quant = \"8bit\"\n",
    "    else:\n",
    "        recommended_model = \"phi-2\"\n",
    "        recommended_quant = \"4bit\"\n",
    "    \n",
    "    print(f\"• Model: {recommended_model}\")\n",
    "    print(f\"• Quantization: {recommended_quant}\")\n",
    "    print(f\"• Batch size: 1\")\n",
    "    \n",
    "    # Initialize experiment configuration with recommended values\n",
    "    experiment_config = {\n",
    "        \"model_name\": recommended_model,\n",
    "        \"fields\": [\"invoice_number\", \"invoice_date\", \"total_amount\", \"vendor_name\"],\n",
    "        \"batch_size\": 1,\n",
    "        \"memory_optimization\": True,\n",
    "        \"quantization\": recommended_quant\n",
    "    }\n",
    "    \n",
    "    # Display initial configuration\n",
    "    print(\"\\n⚙️ Current experiment configuration:\")\n",
    "    for key, value in experiment_config.items():\n",
    "        if isinstance(value, list):\n",
    "            print(f\"• {key}: {', '.join(value)}\")\n",
    "        else:\n",
    "            print(f\"• {key}: {value}\")\n",
    "    \n",
    "    # Interactive UI for experiment configuration\n",
    "    try:\n",
    "        # Import widgets\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        print(\"\\n💡 Use the controls below to customize your experiment:\")\n",
    "        \n",
    "        # Experiment type selection\n",
    "        experiment_type_dropdown = widgets.Dropdown(\n",
    "            options=[\"basic\", \"model_comparison\", \"prompt_comparison\", \"quantization_comparison\", \"custom\"],\n",
    "            value=\"basic\",\n",
    "            description='Experiment type:',\n",
    "        )\n",
    "        \n",
    "        # Model selection\n",
    "        model_dropdown = widgets.Dropdown(\n",
    "            options=available_models,\n",
    "            value=recommended_model,\n",
    "            description='Model:',\n",
    "        )\n",
    "        \n",
    "        # Field selection\n",
    "        field_select = widgets.SelectMultiple(\n",
    "            options=list(fields.keys()),\n",
    "            value=[\"invoice_number\", \"invoice_date\", \"total_amount\"],\n",
    "            description='Fields:',\n",
    "            layout=widgets.Layout(height='120px')\n",
    "        )\n",
    "        \n",
    "        # Quantization selection\n",
    "        quant_dropdown = widgets.Dropdown(\n",
    "            options=[\"float32\", \"none\", \"bfloat16\", \"8bit\", \"4bit\"],\n",
    "            value=recommended_quant,\n",
    "            description='Quantization:',\n",
    "        )\n",
    "        \n",
    "        # Batch size\n",
    "        batch_slider = widgets.IntSlider(\n",
    "            value=1,\n",
    "            min=1,\n",
    "            max=10,\n",
    "            step=1,\n",
    "            description='Batch size:',\n",
    "        )\n",
    "        \n",
    "        # Memory optimization\n",
    "        memory_checkbox = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Memory optimization',\n",
    "        )\n",
    "        \n",
    "        # Create container for custom parameters\n",
    "        custom_params_container = widgets.VBox([])\n",
    "        custom_params = []\n",
    "        \n",
    "        def add_custom_param(b):\n",
    "            \"\"\"Add a custom parameter input group\"\"\"\n",
    "            param_name = widgets.Text(description=\"Name:\", placeholder=\"parameter_name\")\n",
    "            param_value = widgets.Text(description=\"Value:\", placeholder=\"value\")\n",
    "            remove_btn = widgets.Button(description=\"❌\", layout=widgets.Layout(width='40px'))\n",
    "            \n",
    "            param_box = widgets.HBox([param_name, param_value, remove_btn])\n",
    "            custom_params.append((param_name, param_value, param_box))\n",
    "            custom_params_container.children = list(custom_params_container.children) + [param_box]\n",
    "            \n",
    "            def remove_param(b):\n",
    "                custom_params.remove((param_name, param_value, param_box))\n",
    "                custom_params_container.children = [box for _, _, box in custom_params]\n",
    "                update_config()  # Update config after removing a parameter\n",
    "                \n",
    "            remove_btn.on_click(remove_param)\n",
    "        \n",
    "        # Add custom parameter button\n",
    "        add_param_button = widgets.Button(\n",
    "            description=\"Add Custom Parameter\",\n",
    "            button_style='info',\n",
    "            icon='plus'\n",
    "        )\n",
    "        add_param_button.on_click(add_custom_param)\n",
    "        \n",
    "        # Custom parameters section\n",
    "        custom_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h4 style='margin-top:15px;'>Custom Parameters</h4>\"),\n",
    "            widgets.HTML(\"<p style='font-size:0.9em;color:#666;'>Add any custom parameters needed for your experiment:</p>\"),\n",
    "            add_param_button,\n",
    "            custom_params_container\n",
    "        ])\n",
    "        \n",
    "        # Initially hide custom section\n",
    "        custom_section.layout.display = 'none'\n",
    "        \n",
    "        # Show/hide custom section based on experiment type\n",
    "        def update_custom_section(change):\n",
    "            if change['new'] == 'custom':\n",
    "                custom_section.layout.display = 'block'\n",
    "            else:\n",
    "                custom_section.layout.display = 'none'\n",
    "        \n",
    "        experiment_type_dropdown.observe(update_custom_section, names='value')\n",
    "        \n",
    "        # Output area for configuration display\n",
    "        config_output = widgets.Output()\n",
    "        \n",
    "        # Global variable for experiment type\n",
    "        experiment_type = \"basic\"\n",
    "        \n",
    "        def update_config(change=None):\n",
    "            global experiment_type\n",
    "            experiment_type = experiment_type_dropdown.value\n",
    "            \n",
    "            # Update basic configuration\n",
    "            experiment_config[\"model_name\"] = model_dropdown.value\n",
    "            experiment_config[\"fields\"] = list(field_select.value)\n",
    "            experiment_config[\"quantization\"] = quant_dropdown.value\n",
    "            experiment_config[\"batch_size\"] = batch_slider.value\n",
    "            experiment_config[\"memory_optimization\"] = memory_checkbox.value\n",
    "            \n",
    "            # Handle custom parameters for custom experiment type\n",
    "            if experiment_type == \"custom\":\n",
    "                experiment_config[\"custom_parameters\"] = {}\n",
    "                for name_widget, value_widget, _ in custom_params:\n",
    "                    param_name = name_widget.value.strip()\n",
    "                    param_value = value_widget.value.strip()\n",
    "                    \n",
    "                    # Skip empty parameters\n",
    "                    if not param_name:\n",
    "                        continue\n",
    "                        \n",
    "                    # Try to convert to appropriate type\n",
    "                    try:\n",
    "                        # Try as number\n",
    "                        if param_value.isdigit():\n",
    "                            param_value = int(param_value)\n",
    "                        elif param_value.replace('.', '', 1).isdigit():\n",
    "                            param_value = float(param_value)\n",
    "                        # Try as boolean\n",
    "                        elif param_value.lower() in ('true', 'false'):\n",
    "                            param_value = param_value.lower() == 'true'\n",
    "                    except:\n",
    "                        # Keep as string if parsing fails\n",
    "                        pass\n",
    "                        \n",
    "                    experiment_config[\"custom_parameters\"][param_name] = param_value\n",
    "            elif \"custom_parameters\" in experiment_config:\n",
    "                # Remove custom parameters if not a custom experiment\n",
    "                del experiment_config[\"custom_parameters\"]\n",
    "            \n",
    "            # Display updated configuration\n",
    "            with config_output:\n",
    "                config_output.clear_output()\n",
    "                print(\"\\n⚙️ Updated experiment configuration:\")\n",
    "                print(f\"• Type: {experiment_type}\")\n",
    "                for key, value in experiment_config.items():\n",
    "                    if key == \"custom_parameters\":\n",
    "                        print(f\"• {key}:\")\n",
    "                        for param_name, param_value in value.items():\n",
    "                            print(f\"    - {param_name}: {param_value}\")\n",
    "                    elif isinstance(value, list):\n",
    "                        print(f\"• {key}: {', '.join(map(str, value))}\")\n",
    "                    else:\n",
    "                        print(f\"• {key}: {value}\")\n",
    "        \n",
    "        # Register observers\n",
    "        experiment_type_dropdown.observe(update_config, names='value')\n",
    "        model_dropdown.observe(update_config, names='value')\n",
    "        field_select.observe(update_config, names='value')\n",
    "        quant_dropdown.observe(update_config, names='value')\n",
    "        batch_slider.observe(update_config, names='value')\n",
    "        memory_checkbox.observe(update_config, names='value')\n",
    "        \n",
    "        # Create UI sections\n",
    "        basic_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Basic Parameters</h4>\"),\n",
    "            experiment_type_dropdown,\n",
    "            model_dropdown,\n",
    "            field_select,\n",
    "            quant_dropdown,\n",
    "            batch_slider,\n",
    "            memory_checkbox\n",
    "        ])\n",
    "        \n",
    "        # Display UI components\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Experiment Configuration</h3>\"),\n",
    "            basic_section,\n",
    "            custom_section,\n",
    "            widgets.HTML(\"<h4 style='margin-top:15px;'>Configuration Summary</h4>\"),\n",
    "            config_output\n",
    "        ]))\n",
    "        \n",
    "        # Initialize configuration display\n",
    "        update_config()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n⚠️ Interactive widgets not available. Using static configuration instead.\")\n",
    "        print(\"To enable interactive configuration, install ipywidgets: pip install ipywidgets\")\n",
    "        \n",
    "else:\n",
    "    print(\"Utilities not available, cannot display configuration options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b03de48",
   "metadata": {},
   "source": [
    "## 3. Create and Run the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f121b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils_available:\n",
    "    try:\n",
    "        # Create the experiment based on type\n",
    "        print(f\"🔬 Creating {experiment_type} extraction experiment...\")\n",
    "        \n",
    "        # Prepare quantization configuration\n",
    "        quant_config = None\n",
    "        if experiment_config[\"quantization\"] == \"float32\":\n",
    "            quant_config = {\"torch_dtype\": \"float32\"}\n",
    "        elif experiment_config[\"quantization\"] == \"bfloat16\":\n",
    "            quant_config = {\"torch_dtype\": \"bfloat16\"}\n",
    "        elif experiment_config[\"quantization\"] == \"8bit\":\n",
    "            quant_config = {\"bits\": 8, \"use_double_quant\": True}\n",
    "        elif experiment_config[\"quantization\"] == \"4bit\":\n",
    "            quant_config = {\"bits\": 4, \"use_double_quant\": True}\n",
    "        \n",
    "        # Create experiment object based on type\n",
    "        if experiment_type == \"basic\":\n",
    "            experiment = create_basic_experiment(\n",
    "                model_name=experiment_config[\"model_name\"],\n",
    "                fields=experiment_config[\"fields\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                memory_optimization=experiment_config[\"memory_optimization\"],\n",
    "                quantization=quant_config\n",
    "            )\n",
    "            print(f\"✅ Created basic experiment with model {experiment.model_name}\")\n",
    "            \n",
    "        elif experiment_type == \"model_comparison\":\n",
    "            # In the enhanced version, we get models from the UI or use defaults\n",
    "            models_to_compare = experiment_config.get(\"models_to_compare\", [\n",
    "                \"phi-2\", \n",
    "                \"llava-1.5-7b\"\n",
    "            ])\n",
    "            \n",
    "            experiment = create_model_comparison_experiment(\n",
    "                model_names=models_to_compare,\n",
    "                fields=experiment_config[\"fields\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                memory_optimization=experiment_config[\"memory_optimization\"]\n",
    "            )\n",
    "            print(f\"✅ Created model comparison experiment with models: {', '.join(experiment.models_to_compare)}\")\n",
    "            \n",
    "        elif experiment_type == \"prompt_comparison\":\n",
    "            # Get prompt variants from config or create defaults\n",
    "            prompt_variants = experiment_config.get(\"prompt_variants\", {\n",
    "                \"simple\": {field: f\"Extract the {field} from this invoice.\" for field in experiment_config[\"fields\"]},\n",
    "                \"detailed\": {field: f\"Extract the {field} from this invoice. Look for text labeled '{field.replace('_', ' ')}' or similar.\" for field in experiment_config[\"fields\"]}\n",
    "            })\n",
    "            \n",
    "            experiment = create_prompt_comparison_experiment(\n",
    "                model_name=experiment_config[\"model_name\"],\n",
    "                fields=experiment_config[\"fields\"],\n",
    "                prompt_variants=prompt_variants,\n",
    "                batch_size=experiment_config[\"batch_size\"]\n",
    "            )\n",
    "            print(f\"✅ Created prompt comparison experiment with variants: {', '.join(prompt_variants.keys())}\")\n",
    "            \n",
    "        elif experiment_type == \"quantization_comparison\":\n",
    "            # Define quantization strategies\n",
    "            strategies = experiment_config.get(\"quantization_strategies\", [])\n",
    "            \n",
    "            # If no strategies defined in config, create based on hardware\n",
    "            if not strategies:\n",
    "                # Only include full precision if enough memory\n",
    "                if gpu_memory > 40:\n",
    "                    strategies.append({\"torch_dtype\": \"float32\"})\n",
    "                    \n",
    "                if gpu_memory > 16:  # Only include 'none' if enough memory\n",
    "                    strategies.append(None)\n",
    "                \n",
    "                # Check for bfloat16 support\n",
    "                has_bfloat16_support = False\n",
    "                try:\n",
    "                    has_bfloat16_support = torch.cuda.get_device_capability()[0] >= 8\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                if has_bfloat16_support:\n",
    "                    strategies.append({\"torch_dtype\": \"bfloat16\"})\n",
    "                    \n",
    "                strategies.append({\"bits\": 8, \"use_double_quant\": True})\n",
    "                strategies.append({\"bits\": 4, \"use_double_quant\": True})\n",
    "            \n",
    "            experiment = create_quantization_experiment(\n",
    "                model_name=experiment_config[\"model_name\"],\n",
    "                fields=experiment_config[\"fields\"],\n",
    "                quantization_strategies=strategies,\n",
    "                batch_size=experiment_config[\"batch_size\"]\n",
    "            )\n",
    "            print(f\"✅ Created quantization comparison experiment with {len(strategies)} strategies\")\n",
    "            \n",
    "        elif experiment_type == \"custom\":\n",
    "            # Import the custom experiment creation function\n",
    "            try:\n",
    "                from src.notebook.experiment_utils import create_custom_experiment\n",
    "            except ImportError:\n",
    "                # If not yet implemented, we'll create a quick implementation here\n",
    "                from src.config.experiment_config import create_experiment_config, ExperimentType\n",
    "                \n",
    "                def create_custom_experiment(model_name, fields, **custom_params):\n",
    "                    \"\"\"Create a custom experiment with flexible parameters.\"\"\"\n",
    "                    params = {\n",
    "                        \"model_name\": model_name,\n",
    "                        \"fields_to_extract\": fields,\n",
    "                        **custom_params\n",
    "                    }\n",
    "                    return create_experiment_config(\n",
    "                        experiment_type=ExperimentType.CUSTOM \n",
    "                        if hasattr(ExperimentType, \"CUSTOM\") \n",
    "                        else \"custom\",\n",
    "                        **params\n",
    "                    )\n",
    "            \n",
    "            # Extract custom parameters from config\n",
    "            custom_params = experiment_config.get(\"custom_parameters\", {})\n",
    "            \n",
    "            # Create the custom experiment\n",
    "            experiment = create_custom_experiment(\n",
    "                model_name=experiment_config[\"model_name\"],\n",
    "                fields=experiment_config[\"fields\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                memory_optimization=experiment_config[\"memory_optimization\"],\n",
    "                quantization=quant_config,\n",
    "                **custom_params  # Add all custom parameters\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Created custom experiment with model {experiment.model_name}\")\n",
    "            if custom_params:\n",
    "                print(f\"   Custom parameters: {', '.join(custom_params.keys())}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown experiment type: {experiment_type}\")\n",
    "        \n",
    "        print(f\"\\n📋 Experiment details:\")\n",
    "        print(f\"• Name: {experiment.name}\")\n",
    "        print(f\"• Type: {experiment.experiment_type}\")\n",
    "        print(f\"• Fields: {', '.join(experiment.fields if hasattr(experiment, 'fields') else experiment.fields_to_extract)}\")\n",
    "        \n",
    "        # Create a run button instead of hardcoded True\n",
    "        try:\n",
    "            import ipywidgets as widgets\n",
    "            from IPython.display import display\n",
    "            \n",
    "            run_button = widgets.Button(\n",
    "                description=\"Run Experiment\",\n",
    "                button_style='success',\n",
    "                icon='rocket'\n",
    "            )\n",
    "            \n",
    "            output_area = widgets.Output()\n",
    "            display(widgets.VBox([run_button, output_area]))\n",
    "            \n",
    "            def on_run_clicked(b):\n",
    "                with output_area:\n",
    "                    output_area.clear_output()\n",
    "                    run_experiment(True)\n",
    "            \n",
    "            run_button.on_click(on_run_clicked)\n",
    "            \n",
    "            # Define function to run experiment\n",
    "            def run_experiment(run_now=False):\n",
    "                if run_now:\n",
    "                    print(f\"\\n🚀 Running experiment: {experiment.name}\")\n",
    "                    print(f\"This might take a few minutes, especially if models need to be downloaded.\")\n",
    "                    \n",
    "                    # Start timer\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # Run the experiment\n",
    "                    data_path = experiment_config.get(\"data_path\", os.environ.get(\"DATA_DIR\", \"data\"))\n",
    "                    result = run_extraction_experiment(\n",
    "                        config=experiment,\n",
    "                        data_path=data_path,\n",
    "                        show_progress=True\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate runtime\n",
    "                    runtime = time.time() - start_time\n",
    "                    \n",
    "                    print(f\"✅ Experiment completed in {runtime:.2f} seconds!\")\n",
    "                    print(f\"📊 Processed {len(result.extractions) if hasattr(result, 'extractions') else 'multiple'} invoices\")\n",
    "                    \n",
    "                    # Save the experiment ID for later reference\n",
    "                    experiment_id = result.experiment_id if hasattr(result, 'experiment_id') else f\"{experiment.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                    print(f\"💾 Results saved with ID: {experiment_id}\")\n",
    "                    \n",
    "                    # Display experiment variables for future reference\n",
    "                    print(\"\\n📋 Experiment variables (for replication):\")\n",
    "                    print(f\"experiment_type = '{experiment_type}'\")\n",
    "                    print(f\"experiment_config = {json.dumps(experiment_config, indent=2)}\")\n",
    "                    \n",
    "                    return result\n",
    "                else:\n",
    "                    print(\"\\n⏸️ Experiment ready but not running. Click the 'Run Experiment' button to execute.\")\n",
    "            \n",
    "            # Initial message\n",
    "            with output_area:\n",
    "                print(\"\\n⏸️ Experiment ready. Click the 'Run Experiment' button to execute.\")\n",
    "                \n",
    "        except ImportError:\n",
    "            # Fallback to non-interactive mode\n",
    "            run_now = True  # In a real notebook, this would be a user input via a button\n",
    "            \n",
    "            if run_now:\n",
    "                print(f\"\\n🚀 Running experiment: {experiment.name}\")\n",
    "                print(f\"This might take a few minutes, especially if models need to be downloaded.\")\n",
    "                \n",
    "                # Start timer\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Run the experiment\n",
    "                result = run_extraction_experiment(\n",
    "                    config=experiment,\n",
    "                    data_path=os.environ.get(\"DATA_DIR\", \"data\"),\n",
    "                    show_progress=True\n",
    "                )\n",
    "                \n",
    "                # Calculate runtime\n",
    "                runtime = time.time() - start_time\n",
    "                \n",
    "                print(f\"✅ Experiment completed in {runtime:.2f} seconds!\")\n",
    "                print(f\"📊 Processed {len(result.extractions) if hasattr(result, 'extractions') else 'multiple'} invoices\")\n",
    "                \n",
    "                # Save the experiment ID for later reference\n",
    "                experiment_id = result.experiment_id if hasattr(result, 'experiment_id') else f\"{experiment.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                print(f\"💾 Results saved with ID: {experiment_id}\")\n",
    "                \n",
    "                # Display experiment variables for future reference\n",
    "                print(\"\\n📋 Experiment variables (for replication):\")\n",
    "                print(f\"experiment_type = '{experiment_type}'\")\n",
    "                print(f\"experiment_config = {json.dumps(experiment_config, indent=2)}\")\n",
    "            else:\n",
    "                print(\"\\n⏸️ Experiment ready but not running. Execute the cell again with run_now=True to execute.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating/running experiment: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Utilities not available, cannot create or run experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a57f7d",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "Explore the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will display results when an experiment is run\n",
    "if 'result' in locals():\n",
    "    print(\"📊 Experiment Results\")\n",
    "    print(\"==================\")\n",
    "    \n",
    "    # For basic experiment\n",
    "    if hasattr(result, 'extractions'):\n",
    "        print(f\"Total extractions: {len(result.extractions)}\")\n",
    "        \n",
    "        # Create a DataFrame for better viewing\n",
    "        results_data = []\n",
    "        for extraction in result.extractions[:10]:  # Show first 10\n",
    "            row = {\n",
    "                \"document_id\": extraction.document_id,\n",
    "            }\n",
    "            # Add extracted fields\n",
    "            for field, value in extraction.extracted_fields.items():\n",
    "                row[field] = value\n",
    "            \n",
    "            # Add accuracy if available\n",
    "            if hasattr(extraction, 'accuracy'):\n",
    "                row['accuracy'] = extraction.accuracy\n",
    "            \n",
    "            results_data.append(row)\n",
    "        \n",
    "        if results_data:\n",
    "            df = pd.DataFrame(results_data)\n",
    "            print(\"\\n📋 Sample extraction results:\")\n",
    "            display(df)\n",
    "        \n",
    "        # Show overall metrics if available\n",
    "        if hasattr(result, 'metrics'):\n",
    "            print(\"\\n📈 Overall metrics:\")\n",
    "            for metric, value in result.metrics.items():\n",
    "                print(f\"• {metric}: {value}\")\n",
    "    \n",
    "    # For model comparison\n",
    "    elif hasattr(result, 'model_results'):\n",
    "        print(\"Model comparison results:\")\n",
    "        model_metrics = []\n",
    "        for model_name, model_result in result.model_results.items():\n",
    "            model_metrics.append({\n",
    "                \"model\": model_name,\n",
    "                \"accuracy\": getattr(model_result, 'accuracy', 'N/A'),\n",
    "                \"processing_time\": getattr(model_result, 'processing_time', 'N/A'),\n",
    "                \"extractions\": len(model_result.extractions) if hasattr(model_result, 'extractions') else 'N/A'\n",
    "            })\n",
    "        \n",
    "        if model_metrics:\n",
    "            df = pd.DataFrame(model_metrics)\n",
    "            print(\"\\n📊 Model comparison:\")\n",
    "            display(df)\n",
    "    \n",
    "    # For prompt comparison\n",
    "    elif hasattr(result, 'prompt_results'):\n",
    "        print(\"Prompt comparison results:\")\n",
    "        prompt_metrics = []\n",
    "        for prompt_name, prompt_result in result.prompt_results.items():\n",
    "            prompt_metrics.append({\n",
    "                \"prompt\": prompt_name,\n",
    "                \"accuracy\": getattr(prompt_result, 'accuracy', 'N/A'),\n",
    "                \"extractions\": len(prompt_result.extractions) if hasattr(prompt_result, 'extractions') else 'N/A'\n",
    "            })\n",
    "        \n",
    "        if prompt_metrics:\n",
    "            df = pd.DataFrame(prompt_metrics)\n",
    "            print(\"\\n📊 Prompt comparison:\")\n",
    "            display(df)\n",
    "    \n",
    "    # For quantization comparison\n",
    "    elif hasattr(result, 'quantization_results'):\n",
    "        print(\"Quantization comparison results:\")\n",
    "        quant_metrics = []\n",
    "        for quant_name, quant_result in result.quantization_results.items():\n",
    "            quant_metrics.append({\n",
    "                \"quantization\": quant_name,\n",
    "                \"accuracy\": getattr(quant_result, 'accuracy', 'N/A'),\n",
    "                \"memory_usage\": getattr(quant_result, 'memory_usage', 'N/A'),\n",
    "                \"processing_time\": getattr(quant_result, 'processing_time', 'N/A')\n",
    "            })\n",
    "        \n",
    "        if quant_metrics:\n",
    "            df = pd.DataFrame(quant_metrics)\n",
    "            print(\"\\n📊 Quantization comparison:\")\n",
    "            display(df)\n",
    "    \n",
    "    # Create visualizations if available\n",
    "    try:\n",
    "        print(\"\\n📈 Generating visualizations...\")\n",
    "        viz = visualize_experiment_results(result, output_format=\"notebook\")\n",
    "        display(viz)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not generate visualizations: {str(e)}\")\n",
    "    \n",
    "    # Provide code for loading these results later\n",
    "    print(\"\\n💾 Load these results later with:\")\n",
    "    print(f\"\"\"\n",
    "    from src.notebook.experiment_utils import load_experiment_results\n",
    "    result, metadata = load_experiment_results(\"{experiment_id}\")\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"No experiment results available. Run an experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0616c29",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "- Run different experiment types to compare models, prompts, or quantization strategies\n",
    "- Load previous experiment results for detailed analysis\n",
    "- Use the Results Analysis notebook to compare multiple experiments\n",
    "- Create custom prompt templates for better extraction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Experiment complete!\")\n",
    "print(\"To run another experiment, modify the configuration above and re-run the cells.\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
